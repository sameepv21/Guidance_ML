# Introduction
* Predicting few future words are easier as compared to whole sentences.
* Hence, a model that assigns a probability to each possible next word can be very useful in speech recognition.
* Probabilities are also important for augmentative and alternative communication systems.
* Models that assign probabilities to sequence of words are called language models or LMs.
* N-Gram is the simplest model that assigns probabilities to secntences and sequene of words.

# N-Grams
* We denote P(w | h) as "pw|h" which means probability that the next word is w given the history h.
* One way to estimate this probability is from relative frequency i.e find total counts of h and total counts of h followed by w.
* Works fine in many cases but similar sentence might be ignored resulting into count being 0.
* Furthermore, counting can be huge if individual words are taken into account.
* Simplification and cleverer method is using chain rule of probability.
* The chain rule shows the link between cimputing the joint probability of a sequence and computing the conditional probability of a word given previous words.
* ![](./assets/images/2022-07-19-12-47-11.png)
* However, the problem with this approach is that we do not know exact probability of a word given a long sequence of preceding words.
* **The intuition of n-gram model is that instead of computing the probabiliy of a word given its entire history, we can appoximate the history by just the last few words.**
* For example, for bigram model, the approximation looks something as follows
    * ![](./assets/images/2022-07-19-12-49-55.png)
* This assumption is also called a markov assumption.
* To estimate n-gram probabilities, we use MLE.
* We get MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.
* ![](./assets/images/2022-07-19-12-54-20.png)
* For true probability distribution, usually start and end symbol are added.
* Start symbol is added for n-1 times and end symbol added only once. (\<s> and \</s>)
* Thus, n-gram probabilities are calculated by dividing the obsereved frequency of a particular sequence by the observed frequency of a prefix. (Also called relative frequency)
* These probabilities (esp for bigrams) aencode some syntactice facts.

# Issues
* More you multiply probabilities, the smaller it gets and hence to solve that, we use log probabilities.
* The above phenomena where raw probability multiplications results in so small numbers might result into numerical underflow where a program can't store infinitestimally small values.