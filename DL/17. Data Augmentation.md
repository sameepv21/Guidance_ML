# Introduction
* Data Augmentation is a set of techniques used to increase the amount of data in a machine learning model by adding slightly modified copies of already existing data or newly created synthetic data from existing data.
* Helps smooth out the ML model and reduce overfitting of data.

# Advantages
* Performance Improves
* Overfitting reduces
* Cost effective method as collecting more data can become costly

# Disadvantages
* No new information is passed to the model except in GANs.

# Techniques
* GAN Based
* Geometric transformations
* Elastic transformations
* Flipping
* Color modification
* Cropping
* Rotation
* Noise Injection
* Zoom and scaling
* Brightness
* Random Erasing
* Numerical
    * SMOTE
    * SMOTE NC

# GAN Based
* Basically, majority of the other techniques used for data augmentation do not create/add new data to the exisiting dataset.
* They simply modify the existing data such as flipping or rotation.
* They, do introduce a different perspective for the CNN to identify images.
* However, they do not add diversity to the exising data.
* Hence, GANs are one of the most widely used data augmentation technique not just for images, but also used for texts too.
* Types of GANs
    * GAN - Generative Adversarial Network
    * DCGAN - Deep Convolutional Generative Adversarial Network
    * WGAN - Wasserstein Generative Adversarial Network
    * CGAN - Conditional Generative Adversarial Network
* GAN Architecture
    * ![](/assets/images/2022-06-22-08-12-47.png)
* Basic Principle of GAN is competition between two neural networks.
* These two neural networks are called Generator (G) and Discriminator (D).
* The job of Generator is to learn about the distributions of the input dataset (may contain corpora or images etc).
* As a result of this, the Generator can "generate" and image that resembles the input images.
* This as well the images from the dataset is passed to the Discriminator.
* The job of Discriminator is to identify/classify whether the image provided was from Generator or the actual dataset.
* Hence, there is a constant competition between Generator and Discriminator where the former tries to maximize the Discriminator's estimate of the probability that a fake instance is real and the latter tries to maximize the probability of assigning correct label to both training and samples from G.
* Also note that during G training, D does not train and during D training, G does not train. This is because, the complexity will increase as we would be trying to hit a moving target.
* Thus, intuitively, there is a need for two loss functions.
* These two loss functions are aggregated into minimax loss function as shown below.
    * ![](/assets/images/2022-06-22-07-55-13.png)
* The two loss function result into a saddle point viz maximum for one loss function and minimum for the other.* Further, the global optimum converges to a point where pdata = pg where pdata = probability of image being from the actual dataset and pg = probability of image generated from G.
* For more Details, look at the paper Generative Adversarial Networks by Ian Goodfellow.
* [Google Explanation of GAN](https://developers.google.com/machine-learning/gan/gan_structure)

# Geometric Transformations
* Flipping just means mirroring the image i.e either left-right or upside-down.
* Color modification means we can convert an rgd image to grayscale.
* Cropping means we crop to a certain part of the image.
* Rotation just means we rotate the image at various degrees.

# Elastic Transformations
* 

# Others
* Noise injection means we randomly inject gaussian noise.
* Zoom/Scaling means either we zoom in to a specific region of image or scale down individual pixel values.
* Brightness means that we can change the brightness of the image (either increase or decrease).