# Introduction
* Suppose you want to classify not as cat and not cat but as cat, dog, koala, chick etc.
* This is a multiclass classification problem.
* Thus, to make this successfull, the last layer of the neural network needs to have number of hidden units equal to number of classes.
* Each of these hidden units will compute the probabilities for each class given the input features.
* To do this, the final output layer is called softmax layer.
* This means that we would use softmax activation function at the last layer.

# Training softmax
* Softmax regression generalized logistic regression to C classes.
* Hence, if c = 2, softmax is essentially logistic regression.
* Loss function
    * Use multiclass log loss or cross entropy.
    * This reduces down to making the probability largest for the actually correct output.
* Gradient Descent with Softmax
    * The trick here is that the derivative for the last layer turns out to be just the residual error.