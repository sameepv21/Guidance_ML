# Introduction
* There are two problems associated with gradient descent.
* These are exploding gradient problem and vanishing gradient problem.
* This means that for the former one, the value of derivative of cost function might be too might and vice versa for the latter one.
* For small intuition, we can say that if the weight matix are bigger than 1 then the value of derivative might explode and vice versa for vanishing one.
* The partial solution for this is to carefully choose the weights and initialize them.

# Weight Initialization
* Suppose you have a deep neural network.
* The larger the value of n, the lesser the weights you want to be.
    * This is because, eventually at the end, they are going to be added.
    * Hence, it makes sense to make indivual values small, so that sum of them does not *explode*
* One way to achieve this is to set Var(Wi) = 1/n where n is the 
> Must read reason for this and xavier initialization.