# Introduction
* We know that there are multiple hidden layers.
* Each layer has numerous nodes.
* So the process of calculating linear regression output and applying activation to the data provided by previous layer and passing that to the next layer is called forward propagation.
* So the general rule is shown in the image below.
    * ![](/assets/images/2022-06-10-18-56-26.png)

# Circuit Theory and Deep Learning
* Informally it states that these are functions you can compute with a small (meaning that number of hidden units per layer is small and not the number of layers) L-layer deep neural network that shallower networks require exponentially more hidden units to compute.

# Backward Propagation
* The practice of fine-tuning the weights of a neural netowrk based on the error rate obtained in the previous epoch and a method called gradient descent is called backpropagation.

# Important notes
* You need to get the dimensions of matrices right
* Hence, it is always a good idea to write and get the dimensions of the matrices correct.