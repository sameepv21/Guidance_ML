# Introduction
* Regularisation is a process used to solve the problem of overfitting.
* Basically, we add the penalty term to the otherwise perfect loss function.
* It is similar to that done in ridge and lasso regression.
* Here, in deep neural network also, the regularisation is added in the form of frobenium norm with is the sum of squares of all the elements of a matrix.

# Gradient descent
* This is one of the most important backpropagation step.
* Basically, now we need to take into consideration the derivate of penalty term also along with ther regular loss function which calculating the derivative.
* Then, update the weights according to the normal equation used till now.
* This is also the reason that l2 regularisation is called weight decay because the weights are literally getting decayed.

# Regularisation preventing overfitting
* Remember the method of adding regularization is to add a penalty term to the loss function.
* Basically, in the case of deep neural network, what regularization does is that it reduces the weights of many neurons to 0.
* Hence, in the big picture, effectively the deep neural network has a lot less number of hidden units than we would expect.
* As a result, bias is added and variance is reduced.

# Dropout Regularization
* Basically, we either keep a hidden unit or not with some probability.
* With the remaining neurons, we train the model.
* Effectively, this means that we are training on a subset of neurons and regularization happens.
* Further, we repeat this for each training set and train only for a subset of neurons.
* There is another version of this called inverted dropout.
    * Basically in this, the final activations are divided by the threshold probability.
    * The reason for doing this is to ensure that the expected value of the final output remains the same irrespective of number of neurons omitted.
* **Note that at the test time, you dont use this method. The reason for this is that at the test time, you dont require your output to be random.**
* Also, you can keep the threshold for each layer different and hence there is a lot of flexibility.
* One big disadvantage of this is that cost function is not well defined because you are randomly deleting the neurons.

# Other regularizations
* Increase the size of data.
* Data Augmentation
    * This means, that we transform the original data and try to create a new data.
    * This can also mean, you can add some random noise too!
    * For eg. Flipping or rotating an image.
* Early Stopping
    * Read more about the downside called orthogonalization.
    * Advantage includes computationally super efficient.