# Introduction
* There are always a lot of hyperparameters and choosing the correct value of hyperparameters is always challenging.
* One of the methods is using grid search.
* However, this is not done in deep learning because the number of hyperparameters are a lot more and might take more time if grid search is used.
* Instead, we randomly select the hyper parameters and get the approximate optimal value.
* Further, randomness increases the chance of distinct values.

# Coarse to fine scheme
* In this, you select a subsection of the random hyperparameters and zoom in into that region and again run the optimization process.
* This gets a very refined values of hyperparmeters.
* Start with coarse sample and move to fine sample.

# Choosing scale
* For learning rate, use log scale (prefered because uniformly gets chosen)
* For others like number of hidden units or number of hidden layers, use normal scale according to the problem at hand.
* For $\beta$ (in exponentially weighted average), use log scale too!
* Why linear scale does not work?
    * This is because, the value of $\beta$ changes drastically resulting into huge fluctuations.
    * This is not something that we want.